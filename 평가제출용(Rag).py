import os
from dotenv import load_dotenv
from pinecone import Pinecone
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. ì´ˆê¸° ì„¤ì • (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë©´ ë˜ëŠ” ê²ƒë“¤)
load_dotenv(override=True)
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')

# ê°ì²´ë“¤ì„ ë£¨í”„ ë°–ì—ì„œ ë¯¸ë¦¬ ìƒì„±í•´ë‘ë©´ ì†ë„ê°€ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index("lowbirth")
embedding = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)

chat_template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì¡°ìˆ˜ì…ë‹ˆë‹¤. ì œê³µëœ contextì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
    ("human", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ë‚´ìš©(context): {context}"),
])

def search_top_k(question_text):
    # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    embedded_question = embedding.embed_query(question_text)

    # Pinecone ê²€ìƒ‰
    query_result = index.query(
        namespace="lowbirth_1",
        vector=embedded_question,
        top_k=3,
        include_metadata=True
    )

    context_list = []
    for match in query_result.matches:
        if "chunk_text" in match.metadata:
            context_list.append(match.metadata["chunk_text"])
    
    return "\n\n".join(context_list)

# 2. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
if __name__ == "__main__":
    print("=== ì €ì¶œì‚° ê´€ë ¨ RAG ì±„íŒ…ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”) ===")
    
    chain = chat_template | llm | StrOutputParser()

    # ë¬´í•œ ë£¨í”„ ì‹œì‘
    while True:
        print("\n" + "="*50)
        user_question = input("ì§ˆë¬¸ (ì¢…ë£Œ: exit): ").strip()

        # [í•µì‹¬] ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        if user_question.lower() in ['exit', 'quit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
            print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            break # ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ê°€ í”„ë¡œê·¸ë¨ ì¢…ë£Œ

        if not user_question: # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            continue

        # 1ë‹¨ê³„: ê²€ìƒ‰
        print("ğŸ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ëŠ” ì¤‘...")
        top_k_context = search_top_k(user_question)

        # 2ë‹¨ê³„: ë‹µë³€ ìƒì„±
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        response = chain.invoke({
            "question": user_question,
            "context": top_k_context
        })

        print(f"\n[ë‹µë³€]: {response}")